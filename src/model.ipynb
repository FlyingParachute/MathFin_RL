{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coursework for 《Advances in Machine Learning》, 2025 Spring.**\n",
    "***\n",
    "This notebook demonstrates the implementation of [Trading financial indices with reinforcement learning agents](https://doi.org/10.1016/j.eswa.2018.02.032).\n",
    "\n",
    "Our github repository is [here](https://github.com/FlyingParachute/MathFin_RL).\n",
    "\n",
    "**Group members:** Jingtong Xu, Jinyi Lin, Sunqinli Wang, Xingjian Zhao."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Parameter setting (same as the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma   = 0.9\n",
    "lambda_ = 0.9\n",
    "alpha   = 0.1\n",
    "epsilon = 0.01\n",
    "eta     = 0.1\n",
    "\n",
    "column_names = {\n",
    "    'ptf1': {\n",
    "        'quarterly':  {'spx': 'SPXret_1q', 'agg': 'AGGret_1q'},\n",
    "        'semi_annual':{'spx': 'SPXret_s',  'agg': 'AGGret_s'},\n",
    "        'annual':     {'spx': 'SPXret_a',  'agg': 'AGGret_a'}\n",
    "    },\n",
    "    'ptf3': {\n",
    "        'quarterly':  {'spx': 'SPXret_1q', 'agg': 'TNXret_1q'},\n",
    "        'semi_annual':{'spx': 'SPXret_s',  'agg': 'TNXret_s'},\n",
    "        'annual':     {'spx': 'SPXret_a',  'agg': 'TNXret_a'}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Data\n",
    "\n",
    "**Portfolios 1**: the combination of SPX and AGG;\n",
    "\n",
    "**Portfolio 2**: the combination of SPX and T-NOTE 10YR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ptf1_annual():\n",
    "    \"\"\"\n",
    "    Load SPX and AGG data\n",
    "    read Portfolio_1.xlsx annual data and set 'Dates' column as index.\n",
    "    \"\"\"\n",
    "    file_path = './data/processed/Portfolio_1.xlsx'\n",
    "    sheets = ['Quarterly', 'Semi Annually', 'Yearly']\n",
    "    ptf1 = {sheet: pd.read_excel(file_path, sheet_name=sheet) for sheet in sheets}\n",
    "    return ptf1['Yearly'].set_index('Dates')\n",
    "\n",
    "def load_ptf3_annual():\n",
    "    \"\"\"\n",
    "    Load SPX and TNOTE data\n",
    "    read Portfolio_3.xlsx annual data and set 'Dates' column as index.\n",
    "    \"\"\"\n",
    "    file_path = './data/processed/Portfolio_3.xlsx'\n",
    "    sheets = ['Quarterly', 'Semi Annually', 'Yearly']\n",
    "    ptf3 = {sheet: pd.read_excel(file_path, sheet_name=sheet) for sheet in sheets}\n",
    "    return ptf3['Yearly'].set_index('Dates')\n",
    "\n",
    "def ensure_dir_exists(directory):\n",
    "    \"\"\"ensure_dir_exists, if the directory does not exist, create it.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Define the State Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(spx_ret, agg_ret):\n",
    "    \"\"\"Convert positive and negative returns of spx and agg to binary states.\"\"\"\n",
    "    s = ''\n",
    "    s += '1' if spx_ret >= 0 else '0'\n",
    "    s += '1' if agg_ret >= 0 else '0'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Action | 1      | 2      | 3      | 4      | 5      |\n",
    "|--------|--------|--------|--------|--------|--------|\n",
    "| **S&P 500 (%)** | 0      | 25     | 50     | 75     | 100    |\n",
    "| **AGG or T-bill bond (%)** | 100    | 75     | 50     | 25     | 0      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Discrete action space (stock proportion)\n",
    "actions = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "def initialize_q():\n",
    "    \"\"\"Initialize Q matrix (4 states x 5 actions).\"\"\"\n",
    "    states = ['11', '01', '10', '00']\n",
    "    return pd.DataFrame(np.random.rand(4, 5), index=states, columns=actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Train the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaLambdaAgent:\n",
    "    \"\"\"\n",
    "    SARSA(λ) discrete action agent.\n",
    "    reward_type='return' or 'sharpe'.\n",
    "    \"\"\"\n",
    "    def __init__(self, reward_type='return'):\n",
    "        self.q = initialize_q()\n",
    "        self.e = pd.DataFrame(np.zeros((4, 5)), index=self.q.index, columns=self.q.columns)\n",
    "        self.reward_type = reward_type\n",
    "        self.A = 0  # First moment for differential Sharpe ratio\n",
    "        self.B = 0  # Second moment for differential Sharpe ratio\n",
    "\n",
    "    def get_reward(self, spx_ret, agg_ret, action):\n",
    "        portfolio_ret = action * spx_ret + (1 - action) * agg_ret\n",
    "        \n",
    "        if self.reward_type == 'return':\n",
    "            return portfolio_ret\n",
    "        else:  # sharpe\n",
    "            # 使用旧的A、B计算差分夏普比率\n",
    "            old_A = self.A\n",
    "            old_B = self.B\n",
    "            denominator = (old_B - old_A**2)**1.5\n",
    "            \n",
    "            if denominator == 0:\n",
    "                dsr = 0\n",
    "            else:\n",
    "                dsr = (old_B * (portfolio_ret - old_A) - 0.5 * old_A * (portfolio_ret**2 - old_B)) / denominator\n",
    "            \n",
    "            # 更新A、B\n",
    "            self.A = old_A + eta * (portfolio_ret - old_A)\n",
    "            self.B = old_B + eta * (portfolio_ret**2 - old_B)\n",
    "            return dsr\n",
    "\n",
    "    def update(self, state, action, reward, next_state, next_action):\n",
    "        # Calculate TD error\n",
    "        delta = reward + gamma * self.q.loc[next_state, next_action] - self.q.loc[state, action]\n",
    "        \n",
    "        # Replace trace update - set current state-action pair to 1\n",
    "        self.e.loc[state, action] = 1\n",
    "        \n",
    "        # Update Q values\n",
    "        for s in self.q.index:\n",
    "            for a in self.q.columns:\n",
    "                self.q.loc[s, a] += alpha * delta * self.e.loc[s, a]\n",
    "        \n",
    "        # Decay all eligibility traces\n",
    "        self.e = gamma * lambda_ * self.e\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(actions)\n",
    "        else:\n",
    "            return self.q.loc[state].idxmax()\n",
    "\n",
    "class QLambdaAgent(SarsaLambdaAgent):\n",
    "    \"\"\"\n",
    "    Q(λ) discrete action agent.\n",
    "    reward_type='return' or 'sharpe'.\n",
    "    \"\"\"\n",
    "    def __init__(self, reward_type='return'):\n",
    "        super().__init__(reward_type)\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        # Find the action with maximum Q value for the next state\n",
    "        a_star = self.q.loc[next_state].idxmax()\n",
    "        \n",
    "        # Calculate TD error\n",
    "        delta = reward + gamma * self.q.loc[next_state, a_star] - self.q.loc[state, action]\n",
    "        \n",
    "        # Set eligibility trace for the current state-action pair to 1\n",
    "        self.e.loc[state, action] = 1\n",
    "        \n",
    "        # Update Q values\n",
    "        for s in self.q.index:\n",
    "            for a in self.q.columns:\n",
    "                self.q.loc[s, a] += alpha * delta * self.e.loc[s, a]\n",
    "        \n",
    "        # Update eligibility traces based on greedy action\n",
    "        # If the next action is not greedy, zero all eligibility traces\n",
    "        next_action = self.choose_action(next_state)\n",
    "        if next_action != a_star:\n",
    "            self.e = 0 * self.e\n",
    "        else:\n",
    "            self.e = gamma * lambda_ * self.e\n",
    "\n",
    "class TDContinuousAgent:\n",
    "    \"\"\"\n",
    "    TD(λ) continuous action agent (two assets).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Each state has theta=[theta1, theta2], theta1 in [0,1] represents stock proportion\n",
    "        states = ['11', '01', '10', '00']\n",
    "        self.theta = {s: [np.random.uniform(0, 1), 0] for s in states}\n",
    "        self.e_trace = {s: [0, 0] for s in states}\n",
    "\n",
    "    def get_value(self, state, spx_ret, agg_ret):\n",
    "        \"\"\"Calculate the value function for the current state.\"\"\"\n",
    "        # V(s) = θ₁ᴱ(R_t^S - R_t^B) + θ₂ᴱ\n",
    "        return self.theta[state][0] * (spx_ret - agg_ret) + self.theta[state][1]\n",
    "\n",
    "    def get_allocation(self, state):\n",
    "        \"\"\"Return stock allocation proportion based on current state.\"\"\"\n",
    "        if np.random.rand() < epsilon:\n",
    "            # Exploration: return a random value between [0,1]\n",
    "            return np.random.uniform(0, 1)\n",
    "        else:\n",
    "            # Exploitation: return the θ₁ value for the current state\n",
    "            return np.clip(self.theta[state][0], 0, 1)\n",
    "\n",
    "    def update(self, state, spx_ret, agg_ret, reward, next_state):\n",
    "        # Calculate value functions for current and next states\n",
    "        current_value = self.get_value(state, spx_ret, agg_ret)\n",
    "        next_value = self.get_value(next_state, spx_ret, agg_ret)\n",
    "        \n",
    "        # Calculate TD error\n",
    "        delta = reward + gamma * next_value - current_value\n",
    "        \n",
    "        # Update eligibility trace: e = γλe + ∇θV(s)\n",
    "        gradient = [spx_ret - agg_ret, 1]  # ∇θV(s) = (R_t^S - R_t^B, 1)^T\n",
    "        for i in range(2):\n",
    "            self.e_trace[state][i] = gamma * lambda_ * self.e_trace[state][i] + gradient[i]\n",
    "        \n",
    "        # Update parameters: θ = θ + αδe\n",
    "        for i in range(2):\n",
    "            self.theta[state][i] += alpha * delta * self.e_trace[state][i]\n",
    "        \n",
    "        # Constrain θ₁ to the [0,1] interval\n",
    "        self.theta[state][0] = np.clip(self.theta[state][0], 0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Backtest Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(data, train_end_date, test_end_date,\n",
    "             agent_type='sarsa', reward_type='return',\n",
    "             freq='annual', portfolio='ptf1'):\n",
    "    \"\"\"\n",
    "    Static Knowledge Agents (SKAs).\n",
    "    \"\"\"\n",
    "    spx_col = column_names[portfolio][freq]['spx']\n",
    "    agg_col = column_names[portfolio][freq]['agg']\n",
    "\n",
    "    if isinstance(train_end_date, str):\n",
    "        train_end_date = pd.to_datetime(train_end_date)\n",
    "    if isinstance(test_end_date, str):\n",
    "        test_end_date = pd.to_datetime(test_end_date)\n",
    "\n",
    "    train_data = data[data.index <= train_end_date]\n",
    "    test_data = data[(data.index > train_end_date) & (data.index <= test_end_date)]\n",
    "\n",
    "    # Initialize agent\n",
    "    if agent_type == 'continuous':\n",
    "        agent = TDContinuousAgent()\n",
    "    elif agent_type == 'sarsa':\n",
    "        agent = SarsaLambdaAgent(reward_type=reward_type)\n",
    "    elif agent_type == 'qlearning':\n",
    "        agent = QLambdaAgent(reward_type=reward_type)\n",
    "\n",
    "    # Training phase: multiple episodes with randomized initial states\n",
    "    num_episodes = 50\n",
    "    min_ep_len = 4\n",
    "    for _ in range(num_episodes):\n",
    "        # Randomly select starting point\n",
    "        start_idx = np.random.randint(0, max(1, len(train_data) - min_ep_len))\n",
    "        \n",
    "        # Reset eligibility traces\n",
    "        if agent_type == 'continuous':\n",
    "            agent.e_trace = {s: [0, 0] for s in agent.e_trace}\n",
    "        else:\n",
    "            agent.e = pd.DataFrame(np.zeros((4, 5)), index=agent.q.index, columns=agent.q.columns)\n",
    "            \n",
    "        for i in range(start_idx + 1, len(train_data)):\n",
    "            prev_row = train_data.iloc[i - 1]\n",
    "            curr_row = train_data.iloc[i]\n",
    "            state = get_state(prev_row[spx_col], prev_row[agg_col])\n",
    "\n",
    "            if agent_type == 'continuous':\n",
    "                action = agent.get_allocation(state)\n",
    "            else:\n",
    "                action = agent.choose_action(state)\n",
    "\n",
    "            # Calculate reward\n",
    "            if agent_type in ['sarsa', 'qlearning'] and reward_type == 'sharpe':\n",
    "                reward = agent.get_reward(curr_row[spx_col], curr_row[agg_col], action)\n",
    "            else:\n",
    "                portfolio_ret = action * curr_row[spx_col] + (1 - action) * curr_row[agg_col]\n",
    "                reward = portfolio_ret\n",
    "\n",
    "            next_state = get_state(curr_row[spx_col], curr_row[agg_col])\n",
    "\n",
    "            if agent_type == 'sarsa':\n",
    "                next_action = agent.choose_action(next_state)\n",
    "                agent.update(state, action, reward, next_state, next_action)\n",
    "            elif agent_type == 'qlearning':\n",
    "                agent.update(state, action, reward, next_state)\n",
    "            elif agent_type == 'continuous':\n",
    "                agent.update(state, curr_row[spx_col], curr_row[agg_col], reward, next_state)\n",
    "\n",
    "    # Testing phase\n",
    "    portfolio_values = [10000]\n",
    "    current_value = 10000\n",
    "    dates = [train_end_date]\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        if i == 0:\n",
    "            prev_row = train_data.iloc[-1]\n",
    "        else:\n",
    "            prev_row = test_data.iloc[i - 1]\n",
    "        curr_row = test_data.iloc[i]\n",
    "\n",
    "        state = get_state(prev_row[spx_col], prev_row[agg_col])\n",
    "        if agent_type == 'continuous':\n",
    "            action = agent.get_allocation(state)\n",
    "        else:\n",
    "            action = agent.choose_action(state)\n",
    "\n",
    "        ret = action * curr_row[spx_col] + (1 - action) * curr_row[agg_col]\n",
    "        current_value *= (1 + ret)\n",
    "        portfolio_values.append(current_value)\n",
    "        dates.append(test_data.index[i])\n",
    "\n",
    "    return portfolio_values, dates\n",
    "\n",
    "def backtest_AKA(data, train_end_date, test_end_date,\n",
    "                 agent_type='sarsa', reward_type='return',\n",
    "                 freq='annual', portfolio='ptf1'):\n",
    "    \"\"\"\n",
    "    Adaptive Knowledge Agents (AKAs).\n",
    "    \"\"\"\n",
    "    spx_col = column_names[portfolio][freq]['spx']\n",
    "    agg_col = column_names[portfolio][freq]['agg']\n",
    "\n",
    "    full_data = pd.concat([\n",
    "        data[data.index <= train_end_date],\n",
    "        data[(data.index > train_end_date) & (data.index <= test_end_date)]\n",
    "    ])\n",
    "\n",
    "    portfolio_values = [10000]\n",
    "    dates = [train_end_date]\n",
    "\n",
    "    for current_date in full_data[full_data.index > train_end_date].index:\n",
    "        current_train_data = full_data[full_data.index < current_date]\n",
    "\n",
    "        # Reinitialize agent each time\n",
    "        if agent_type == 'continuous':\n",
    "            agent = TDContinuousAgent()\n",
    "        elif agent_type == 'sarsa':\n",
    "            agent = SarsaLambdaAgent(reward_type=reward_type)\n",
    "        elif agent_type == 'qlearning':\n",
    "            agent = QLambdaAgent(reward_type=reward_type)\n",
    "\n",
    "        # Training\n",
    "        num_episodes = 50  # Increase number of iterations\n",
    "        min_ep_len = 4\n",
    "        if len(current_train_data) > min_ep_len:\n",
    "            for _ in range(num_episodes):\n",
    "                start_idx = np.random.randint(0, len(current_train_data) - min_ep_len)\n",
    "                \n",
    "                # Reset eligibility traces\n",
    "                if agent_type == 'continuous':\n",
    "                    agent.e_trace = {s: [0, 0] for s in agent.e_trace}\n",
    "                else:\n",
    "                    agent.e = pd.DataFrame(np.zeros((4, 5)), index=agent.q.index, columns=agent.q.columns)\n",
    "                    \n",
    "                for i in range(start_idx + 1, len(current_train_data)):\n",
    "                    prev_row = current_train_data.iloc[i - 1]\n",
    "                    curr_row = current_train_data.iloc[i]\n",
    "                    state = get_state(prev_row[spx_col], prev_row[agg_col])\n",
    "\n",
    "                    if agent_type == 'continuous':\n",
    "                        action = agent.get_allocation(state)\n",
    "                    else:\n",
    "                        action = agent.choose_action(state)\n",
    "\n",
    "                    if agent_type in ['sarsa', 'qlearning'] and reward_type == 'sharpe':\n",
    "                        reward = agent.get_reward(curr_row[spx_col], curr_row[agg_col], action)\n",
    "                    else:\n",
    "                        portfolio_ret = action * curr_row[spx_col] + (1 - action) * curr_row[agg_col]\n",
    "                        reward = portfolio_ret\n",
    "\n",
    "                    next_state = get_state(curr_row[spx_col], curr_row[agg_col])\n",
    "\n",
    "                    if agent_type == 'sarsa':\n",
    "                        next_action = agent.choose_action(next_state)\n",
    "                        agent.update(state, action, reward, next_state, next_action)\n",
    "                    elif agent_type == 'qlearning':\n",
    "                        agent.update(state, action, reward, next_state)\n",
    "                    elif agent_type == 'continuous':\n",
    "                        agent.update(state, curr_row[spx_col], curr_row[agg_col], reward, next_state)\n",
    "\n",
    "        # Test current point\n",
    "        prev_row = current_train_data.iloc[-1] if len(current_train_data) > 0 else full_data.iloc[0]\n",
    "        current_row = full_data.loc[current_date]\n",
    "        state = get_state(prev_row[spx_col], prev_row[agg_col])\n",
    "\n",
    "        if agent_type == 'continuous':\n",
    "            action = agent.get_allocation(state)\n",
    "        else:\n",
    "            action = agent.choose_action(state)\n",
    "\n",
    "        ret = action * current_row[spx_col] + (1 - action) * current_row[agg_col]\n",
    "        portfolio_values.append(portfolio_values[-1] * (1 + ret))\n",
    "        dates.append(current_date)\n",
    "\n",
    "    return portfolio_values, dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Function for visualizing the backtest results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_benchmarks(data, train_end_date, test_end_date,\n",
    "                         freq='annual', portfolio='ptf1'):\n",
    "    spx_col = column_names[portfolio][freq]['spx']\n",
    "    agg_col = column_names[portfolio][freq]['agg']\n",
    "\n",
    "    test_data = data[(data.index > train_end_date) & (data.index <= test_end_date)]\n",
    "\n",
    "    allocations = {\n",
    "        'A2': 0.25,\n",
    "        'A3': 0.5,\n",
    "        'A4': 0.75,\n",
    "        'Bonds': 0.0,\n",
    "        'Stocks': 1.0\n",
    "    }\n",
    "    # Ceiling strategy\n",
    "    benchmarks = {k: [10000] for k in allocations}\n",
    "    benchmarks['Ceiling'] = [10000]\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        spx_ret = test_data.iloc[i][spx_col]\n",
    "        agg_ret = test_data.iloc[i][agg_col]\n",
    "        for strategy, alloc in allocations.items():\n",
    "            ret = alloc * spx_ret + (1 - alloc) * agg_ret\n",
    "            benchmarks[strategy].append(benchmarks[strategy][-1] * (1 + ret))\n",
    "        best_ret = max(spx_ret, agg_ret)\n",
    "        benchmarks['Ceiling'].append(benchmarks['Ceiling'][-1] * (1 + best_ret))\n",
    "    return benchmarks\n",
    "\n",
    "def annualized_returns(final_val, years):\n",
    "    \"\"\"\n",
    "    final_val: Final portfolio value\n",
    "    years:     Number of investment years\n",
    "    Returns total return and annualized return\n",
    "    \"\"\"\n",
    "    total_ret = (final_val / 10000 - 1) * 100\n",
    "    ann_ret   = ((1 + total_ret/100)**(1/years) - 1) * 100\n",
    "    return total_ret, ann_ret\n",
    "\n",
    "# ================\n",
    "#  Plot Fig.4 & Fig.5\n",
    "# ================\n",
    "def plot_fig4_5(data, train_start_str, train_end_str, test_start_str, test_end_str,\n",
    "                freq='annual', portfolio='ptf1', fig_title_prefix=''):\n",
    "    \"\"\"\n",
    "    Fig.4: On-policy(SARSA) & Continuous\n",
    "    Fig.5: Off-policy(Q-learning)\n",
    "    Training period: [train_start, train_end]\n",
    "    Testing period: (train_end, test_end]\n",
    "    \"\"\"\n",
    "    train_start = pd.to_datetime(train_start_str)\n",
    "    train_end   = pd.to_datetime(train_end_str)\n",
    "    test_start  = pd.to_datetime(test_start_str)\n",
    "    test_end    = pd.to_datetime(test_end_str)\n",
    "\n",
    "    # Extract data for this period\n",
    "    full_data = data[(data.index >= train_start) & (data.index <= test_end)]\n",
    "    # Will filter again in the backtest below\n",
    "    # Here only ensures data is within bounds\n",
    "\n",
    "    # Benchmarks\n",
    "    benchmarks = calculate_benchmarks(full_data, train_end, test_end, freq, portfolio)\n",
    "\n",
    "    # ========== Fig.4: on-policy & continuous ==========\n",
    "    # SKA(R-SKA)\n",
    "    ska_vals, ska_dates = backtest(full_data, train_end, test_end,\n",
    "                                   agent_type='sarsa', reward_type='return',\n",
    "                                   freq=freq, portfolio=portfolio)\n",
    "    # AKA(R-AKA)\n",
    "    aka_vals, aka_dates = backtest_AKA(full_data, train_end, test_end,\n",
    "                                       agent_type='sarsa', reward_type='return',\n",
    "                                       freq=freq, portfolio=portfolio)\n",
    "    # S-SKA\n",
    "    s_ska_vals, _ = backtest(full_data, train_end, test_end,\n",
    "                             agent_type='sarsa', reward_type='sharpe',\n",
    "                             freq=freq, portfolio=portfolio)\n",
    "    # S-AKA\n",
    "    s_aka_vals, _ = backtest_AKA(full_data, train_end, test_end,\n",
    "                                 agent_type='sarsa', reward_type='sharpe',\n",
    "                                 freq=freq, portfolio=portfolio)\n",
    "    # CA-SKA\n",
    "    ca_ska_vals, _ = backtest(full_data, train_end, test_end,\n",
    "                              agent_type='continuous', reward_type='return',\n",
    "                              freq=freq, portfolio=portfolio)\n",
    "    # CA-AKA\n",
    "    ca_aka_vals, _ = backtest_AKA(full_data, train_end, test_end,\n",
    "                                  agent_type='continuous', reward_type='return',\n",
    "                                  freq=freq, portfolio=portfolio)\n",
    "\n",
    "    fig4 = plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ska_dates, ska_vals,   label='SKA (R-SKA)', linewidth=2)\n",
    "    plt.plot(aka_dates, aka_vals,   label='AKA (R-AKA)', linewidth=2)\n",
    "    plt.plot(ska_dates, s_ska_vals, label='S-SKA',       linewidth=2)\n",
    "    plt.plot(ska_dates, s_aka_vals, label='S-AKA',       linewidth=2)\n",
    "    plt.plot(ska_dates, ca_ska_vals,label='CA-SKA',      linewidth=2)\n",
    "    plt.plot(ska_dates, ca_aka_vals,label='CA-AKA',      linewidth=2)\n",
    "    plt.plot(ska_dates, benchmarks['Bonds'],  label='AGG Bonds', linestyle='--')\n",
    "    plt.plot(ska_dates, benchmarks['Stocks'], label='S&P 500',   linestyle='--')\n",
    "    plt.plot(ska_dates, benchmarks['Ceiling'],label='Ceiling',   linestyle='-.')\n",
    "    # Remove logarithmic scale\n",
    "    # plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'{fig_title_prefix}Portfolio Performance: On-policy & Continuous Agents')\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.ylabel('Portfolio Value ($)')\n",
    "    plt.xlabel('Year')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ========== Fig.5: off-policy ==========\n",
    "    # Q-SKA\n",
    "    q_ska_vals, q_ska_dates = backtest(full_data, train_end, test_end,\n",
    "                                       agent_type='qlearning', reward_type='return',\n",
    "                                       freq=freq, portfolio=portfolio)\n",
    "    # Q-AKA\n",
    "    q_aka_vals, _ = backtest_AKA(full_data, train_end, test_end,\n",
    "                                 agent_type='qlearning', reward_type='return',\n",
    "                                 freq=freq, portfolio=portfolio)\n",
    "    # QS-SKA\n",
    "    qs_ska_vals, _ = backtest(full_data, train_end, test_end,\n",
    "                              agent_type='qlearning', reward_type='sharpe',\n",
    "                              freq=freq, portfolio=portfolio)\n",
    "    # QS-AKA\n",
    "    qs_aka_vals, _ = backtest_AKA(full_data, train_end, test_end,\n",
    "                                  agent_type='qlearning', reward_type='sharpe',\n",
    "                                  freq=freq, portfolio=portfolio)\n",
    "\n",
    "    fig5 = plt.figure(figsize=(10, 6))\n",
    "    plt.plot(q_ska_dates, benchmarks['Bonds'],  label='Bonds',  linestyle='--')\n",
    "    plt.plot(q_ska_dates, benchmarks['Stocks'], label='Stocks', linestyle='--')\n",
    "    plt.plot(q_ska_dates, q_ska_vals,   label='Q-SKA',   linewidth=2)\n",
    "    plt.plot(q_ska_dates, q_aka_vals,   label='Q-AKA',   linewidth=2)\n",
    "    plt.plot(q_ska_dates, qs_ska_vals,  label='QS-SKA',  linewidth=2)\n",
    "    plt.plot(q_ska_dates, qs_aka_vals,  label='QS-AKA',  linewidth=2)\n",
    "    plt.plot(q_ska_dates, benchmarks['A2'], label='A2', linestyle=':')\n",
    "    plt.plot(q_ska_dates, benchmarks['A3'], label='A3', linestyle=':')\n",
    "    plt.plot(q_ska_dates, benchmarks['A4'], label='A4', linestyle=':')\n",
    "    # Remove logarithmic scale\n",
    "    # plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'{fig_title_prefix}Portfolio Performance: Off-policy Agents')\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.ylabel('Portfolio Value ($)')\n",
    "    plt.xlabel('Year')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig4, fig5\n",
    "\n",
    "def plot_fig6_7(data, train_start_str, train_end_str, test_start_str, test_end_str,\n",
    "                freq='annual', portfolio='ptf1', fig_title_prefix=''):\n",
    "    \"\"\"\n",
    "    Fig.6: On-policy & Continuous (second training/testing period)\n",
    "    Fig.7: Off-policy (second training/testing period)\n",
    "    \"\"\"\n",
    "    # Same logic as fig4_5, just with a different training/testing period\n",
    "    fig6, fig7 = plot_fig4_5(data, train_start_str, train_end_str, test_start_str, test_end_str,\n",
    "                freq, portfolio, fig_title_prefix='Portfolio Performance: Second Period - ')\n",
    "    \n",
    "    # Rename charts\n",
    "    plt.figure(fig6.number)\n",
    "    plt.title(f'{fig_title_prefix}Portfolio Performance: On-policy & Continuous Agents (Second Period)')\n",
    "    \n",
    "    plt.figure(fig7.number)\n",
    "    plt.title(f'{fig_title_prefix}Portfolio Performance: Off-policy Agents (Second Period)')\n",
    "    \n",
    "    return fig6, fig7\n",
    "\n",
    "\n",
    "# ================\n",
    "#  Plot Fig.8 & Fig.9\n",
    "# ================\n",
    "def plot_fig8_9(data, train_start_str, train_end_str, test_start_str, test_end_str,\n",
    "                freq='annual', portfolio='ptf3', fig_title_prefix=''):\n",
    "    \"\"\"\n",
    "    Fig.8: On-policy(SARSA) & Continuous\n",
    "    Fig.9: Off-policy(Q-learning)\n",
    "    Training period: [train_start, train_end]\n",
    "    Testing period: (train_end, test_end]\n",
    "    \"\"\"\n",
    "    train_start = pd.to_datetime(train_start_str)\n",
    "    train_end   = pd.to_datetime(train_end_str)\n",
    "    test_start  = pd.to_datetime(test_start_str)\n",
    "    test_end    = pd.to_datetime(test_end_str)\n",
    "\n",
    "    # Extract data for this period\n",
    "    full_data = data[(data.index >= train_start) & (data.index <= test_end)]\n",
    "    # Will filter again in the backtest below\n",
    "    # Here only ensures data is within bounds\n",
    "\n",
    "    # Benchmarks\n",
    "    benchmarks = calculate_benchmarks(full_data, train_end, test_end, freq, portfolio)\n",
    "\n",
    "    # ========== Fig.8: on-policy & continuous ==========\n",
    "    # SKA(R-SKA)\n",
    "    ska_vals, ska_dates = backtest(full_data, train_end, test_end,\n",
    "                                   agent_type='sarsa', reward_type='return',\n",
    "                                   freq=freq, portfolio=portfolio)\n",
    "    # AKA(R-AKA)\n",
    "    aka_vals, aka_dates = backtest_AKA(full_data, train_end, test_end,\n",
    "                                       agent_type='sarsa', reward_type='return',\n",
    "                                       freq=freq, portfolio=portfolio)\n",
    "    # S-SKA\n",
    "    s_ska_vals, _ = backtest(full_data, train_end, test_end,\n",
    "                             agent_type='sarsa', reward_type='sharpe',\n",
    "                             freq=freq, portfolio=portfolio)\n",
    "    # S-AKA\n",
    "    s_aka_vals, _ = backtest_AKA(full_data, train_end, test_end,\n",
    "                                 agent_type='sarsa', reward_type='sharpe',\n",
    "                                 freq=freq, portfolio=portfolio)\n",
    "    # CA-SKA\n",
    "    ca_ska_vals, _ = backtest(full_data, train_end, test_end,\n",
    "                              agent_type='continuous', reward_type='return',\n",
    "                              freq=freq, portfolio=portfolio)\n",
    "    # CA-AKA\n",
    "    ca_aka_vals, _ = backtest_AKA(full_data, train_end, test_end,\n",
    "                                  agent_type='continuous', reward_type='return',\n",
    "                                  freq=freq, portfolio=portfolio)\n",
    "\n",
    "    fig8 = plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ska_dates, ska_vals,   label='SKA (R-SKA)', linewidth=2)\n",
    "    plt.plot(aka_dates, aka_vals,   label='AKA (R-AKA)', linewidth=2)\n",
    "    plt.plot(ska_dates, s_ska_vals, label='S-SKA',       linewidth=2)\n",
    "    plt.plot(ska_dates, s_aka_vals, label='S-AKA',       linewidth=2)\n",
    "    plt.plot(ska_dates, ca_ska_vals,label='CA-SKA',      linewidth=2)\n",
    "    plt.plot(ska_dates, ca_aka_vals,label='CA-AKA',      linewidth=2)\n",
    "    plt.plot(ska_dates, benchmarks['Bonds'],  label='T-NOTE', linestyle='--')\n",
    "    plt.plot(ska_dates, benchmarks['Stocks'], label='S&P 500',   linestyle='--')\n",
    "    plt.plot(ska_dates, benchmarks['Ceiling'],label='Ceiling',   linestyle='-.')\n",
    "    # Remove logarithmic scale\n",
    "    # plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'{fig_title_prefix}Portfolio Performance: On-policy & Continuous Agents')\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.ylabel('Portfolio Value ($)')\n",
    "    plt.xlabel('Year')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ========== Fig.9: off-policy ==========\n",
    "    # Q-SKA\n",
    "    q_ska_vals, q_ska_dates = backtest(full_data, train_end, test_end,\n",
    "                                       agent_type='qlearning', reward_type='return',\n",
    "                                       freq=freq, portfolio=portfolio)\n",
    "    # Q-AKA\n",
    "    q_aka_vals, _ = backtest_AKA(full_data, train_end, test_end,\n",
    "                                 agent_type='qlearning', reward_type='return',\n",
    "                                 freq=freq, portfolio=portfolio)\n",
    "    # QS-SKA\n",
    "    qs_ska_vals, _ = backtest(full_data, train_end, test_end,\n",
    "                              agent_type='qlearning', reward_type='sharpe',\n",
    "                              freq=freq, portfolio=portfolio)\n",
    "    # QS-AKA\n",
    "    qs_aka_vals, _ = backtest_AKA(full_data, train_end, test_end,\n",
    "                                  agent_type='qlearning', reward_type='sharpe',\n",
    "                                  freq=freq, portfolio=portfolio)\n",
    "\n",
    "    fig9 = plt.figure(figsize=(10, 6))\n",
    "    plt.plot(q_ska_dates, benchmarks['Bonds'],  label='T-NOTE',  linestyle='--')\n",
    "    plt.plot(q_ska_dates, benchmarks['Stocks'], label='Stocks', linestyle='--')\n",
    "    plt.plot(q_ska_dates, q_ska_vals,   label='Q-SKA',   linewidth=2)\n",
    "    plt.plot(q_ska_dates, q_aka_vals,   label='Q-AKA',   linewidth=2)\n",
    "    plt.plot(q_ska_dates, qs_ska_vals,  label='QS-SKA',  linewidth=2)\n",
    "    plt.plot(q_ska_dates, qs_aka_vals,  label='QS-AKA',  linewidth=2)\n",
    "    plt.plot(q_ska_dates, benchmarks['A2'], label='A2', linestyle=':')\n",
    "    plt.plot(q_ska_dates, benchmarks['A3'], label='A3', linestyle=':')\n",
    "    plt.plot(q_ska_dates, benchmarks['A4'], label='A4', linestyle=':')\n",
    "    # Remove logarithmic scale\n",
    "    # plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'{fig_title_prefix}Portfolio Performance: Off-policy Agents')\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.ylabel('Portfolio Value ($)')\n",
    "    plt.xlabel('Year')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig8, fig9\n",
    "\n",
    "def plot_fig10_11(data, train_start_str, train_end_str, test_start_str, test_end_str,\n",
    "                freq='annual', portfolio='ptf3', fig_title_prefix=''):\n",
    "    \"\"\"\n",
    "    Fig.10: On-policy & Continuous (second training/testing period)\n",
    "    Fig.11: Off-policy (second training/testing period)\n",
    "    \"\"\"\n",
    "    # Same logic as fig8_9, just with a different training/testing period\n",
    "    fig10, fig11 = plot_fig8_9(data, train_start_str, train_end_str, test_start_str, test_end_str,\n",
    "                freq, portfolio, fig_title_prefix='Portfolio Performance: Second Period - ')\n",
    "    \n",
    "    # Rename charts\n",
    "    plt.figure(fig10.number)\n",
    "    plt.title(f'{fig_title_prefix}Portfolio Performance: On-policy & Continuous Agents (Second Period)')\n",
    "    \n",
    "    plt.figure(fig11.number)\n",
    "    plt.title(f'{fig_title_prefix}Portfolio Performance: Off-policy Agents (Second Period)')\n",
    "    \n",
    "    return fig10, fig11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
